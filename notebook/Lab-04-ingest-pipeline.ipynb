{"cells":[{"cell_type":"code","source":["table_name = \"sales\"\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"d2d0d641-454a-4340-8fe9-50c0d40615e5","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-09T21:04:33.1105292Z","session_start_time":null,"execution_start_time":"2024-01-09T21:04:33.7360724Z","execution_finish_time":"2024-01-09T21:04:34.1644241Z","spark_jobs":{"numbers":{"UNKNOWN":0,"SUCCEEDED":0,"FAILED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"ca981151-f0ac-4058-9725-b4495fdf442a"},"text/plain":"StatementMeta(, d2d0d641-454a-4340-8fe9-50c0d40615e5, 5, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"tags":["parameters"]},"id":"2436012b-791c-4ee6-a413-8dd2defa1311"},{"cell_type":"code","source":["from pyspark.sql.functions import *\n","\n","# Read the new sales data\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n","\n","## Add month and year columns\n","df = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n","\n","# Derive FirstName and LastName columns\n","df = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n","\n","# Filter and reorder columns\n","df = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n","\n","# Load the data into a table\n","df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"d2d0d641-454a-4340-8fe9-50c0d40615e5","statement_id":6,"state":"submitted","livy_statement_state":"running","queued_time":"2024-01-09T21:04:33.3156483Z","session_start_time":null,"execution_start_time":"2024-01-09T21:04:34.7360732Z","execution_finish_time":null,"spark_jobs":{"numbers":{"UNKNOWN":0,"SUCCEEDED":7,"FAILED":0,"RUNNING":0},"jobs":[{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":0,"dataRead":4459,"rowCount":50,"usageDescription":"","jobId":17,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 6:\nfrom pyspark.sql.functions import *\n\n# Read the new sales data\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n\n## Add month and year columns\ndf = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n\n# Derive FirstName and LastName columns\ndf = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Filter and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n# Load the data into a table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(table_name): Compute snapshot for version: 0","submissionTime":"2024-01-09T21:04:40.933GMT","completionTime":"2024-01-09T21:04:40.993GMT","stageIds":[24,25,23],"jobGroup":"6","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":4459,"dataRead":2001,"rowCount":54,"usageDescription":"","jobId":16,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 6:\nfrom pyspark.sql.functions import *\n\n# Read the new sales data\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n\n## Add month and year columns\ndf = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n\n# Derive FirstName and LastName columns\ndf = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Filter and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n# Load the data into a table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(table_name): Compute snapshot for version: 0","submissionTime":"2024-01-09T21:04:39.823GMT","completionTime":"2024-01-09T21:04:40.897GMT","stageIds":[21,22],"jobGroup":"6","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":2001,"dataRead":2804,"rowCount":8,"usageDescription":"","jobId":15,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 6:\nfrom pyspark.sql.functions import *\n\n# Read the new sales data\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n\n## Add month and year columns\ndf = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n\n# Derive FirstName and LastName columns\ndf = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Filter and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n# Load the data into a table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(table_name): Compute snapshot for version: 0","submissionTime":"2024-01-09T21:04:39.571GMT","completionTime":"2024-01-09T21:04:39.649GMT","stageIds":[20],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2951","dataWritten":0,"dataRead":2804,"rowCount":4,"usageDescription":"","jobId":14,"name":"toString at String.java:2951","description":"Job group for statement 6:\nfrom pyspark.sql.functions import *\n\n# Read the new sales data\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n\n## Add month and year columns\ndf = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n\n# Derive FirstName and LastName columns\ndf = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Filter and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n# Load the data into a table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)","submissionTime":"2024-01-09T21:04:39.258GMT","completionTime":"2024-01-09T21:04:39.327GMT","stageIds":[19],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":456446,"dataRead":1351449,"rowCount":65436,"usageDescription":"","jobId":13,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Job group for statement 6:\nfrom pyspark.sql.functions import *\n\n# Read the new sales data\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n\n## Add month and year columns\ndf = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n\n# Derive FirstName and LastName columns\ndf = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Filter and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n# Load the data into a table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)","submissionTime":"2024-01-09T21:04:37.156GMT","completionTime":"2024-01-09T21:04:37.815GMT","stageIds":[17,18],"jobGroup":"6","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":1351449,"dataRead":0,"rowCount":65436,"usageDescription":"","jobId":12,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Job group for statement 6:\nfrom pyspark.sql.functions import *\n\n# Read the new sales data\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n\n## Add month and year columns\ndf = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n\n# Derive FirstName and LastName columns\ndf = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Filter and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n# Load the data into a table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)","submissionTime":"2024-01-09T21:04:36.445GMT","completionTime":"2024-01-09T21:04:37.102GMT","stageIds":[16],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"load at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":65536,"rowCount":1,"usageDescription":"","jobId":11,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 6:\nfrom pyspark.sql.functions import *\n\n# Read the new sales data\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n\n## Add month and year columns\ndf = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n\n# Derive FirstName and LastName columns\ndf = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Filter and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n# Load the data into a table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)","submissionTime":"2024-01-09T21:04:35.003GMT","completionTime":"2024-01-09T21:04:35.137GMT","stageIds":[15],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"b3cb5407-385c-4415-bb8d-661018a9f715"},"text/plain":"StatementMeta(, d2d0d641-454a-4340-8fe9-50c0d40615e5, 6, Submitted, Running)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"31d40b10-175b-4df7-8947-3fc469af81b5"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0ff19b7c-a52c-4261-a1f6-6b979fd4ec82"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"host":{},"language":"python"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"4862f3b3-dd67-4a30-8b15-7d76d14f8f6e","known_lakehouses":[{"id":"4862f3b3-dd67-4a30-8b15-7d76d14f8f6e"}],"default_lakehouse_name":"MyLakehouse","default_lakehouse_workspace_id":"5a3d48d7-0be3-418b-bd4e-c28f727bec51"}}},"nbformat":4,"nbformat_minor":5}